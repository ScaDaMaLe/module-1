{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text document processing workflow\n",
    "### Slide 1, page 40-44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.linalg.{Vector, Vectors}\n",
    "import org.apache.spark.ml.param.ParamMap\n",
    "import org.apache.spark.sql.Row\n",
    "\n",
    "case class Article(id: Long, topic: String, text: String)\n",
    "\n",
    "val articles = spark.createDataFrame(Seq(\n",
    "  Article(0, \"sci.math\", \"Hello, Math!\"),\n",
    "  Article(1, \"alt.religion\", \"Hello, Religion!\"),\n",
    "  Article(2, \"sci.physics\", \"Hello, Physics!\"),\n",
    "  Article(3, \"sci.math\", \"Hello, Math Revised!\"),\n",
    "  Article(4, \"sci.math\", \"Better Math\"),\n",
    "  Article(5, \"alt.religion\", \"TGIF\"))).toDF\n",
    "\n",
    "articles.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val topic2Label: Boolean => Double = x => if (x) 1 else 0\n",
    "\n",
    "val toLabel = spark.udf.register(\"topic2Label\", topic2Label)\n",
    "\n",
    "val labelled = articles.withColumn(\"label\", toLabel($\"topic\".like(\"sci%\"))).cache\n",
    "\n",
    "labelled.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.Tokenizer\n",
    "import org.apache.spark.ml.feature.RegexTokenizer\n",
    "\n",
    "val tokenizer = new RegexTokenizer().setInputCol(\"text\").setOutputCol(\"words\")\n",
    "val tokenized = tokenizer.transform(labelled)\n",
    "\n",
    "tokenized.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.HashingTF\n",
    "\n",
    "val hashingTF = new HashingTF().setInputCol(tokenizer.getOutputCol)\n",
    "    .setOutputCol(\"features\")\n",
    "    .setNumFeatures(5000)\n",
    "val hashed = hashingTF.transform(tokenized)\n",
    "\n",
    "hashed.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val Array(trainDF, testDF) = hashed.randomSplit(Array(0.8, 0.2))\n",
    "\n",
    "trainDF.show\n",
    "testDF.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "\n",
    "val lr = new LogisticRegression().setMaxIter(20).setRegParam(0.01)\n",
    "val model = lr.fit(trainDF)\n",
    "val pred = model.transform(testDF).select(\"topic\", \"label\", \"prediction\")\n",
    "\n",
    "pred.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text document processing workflow - Pipeline\n",
    "### Slide 1, page 45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val Array(trainDF2, testDF2) = labelled.randomSplit(Array(0.8, 0.2))\n",
    "\n",
    "trainDF2.show\n",
    "testDF2.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "\n",
    "val pipeline = new Pipeline().setStages(Array(tokenizer, hashingTF, lr))\n",
    "val model2 = pipeline.fit(trainDF2)\n",
    "val pred = model2.transform(testDF2).select(\"topic\", \"label\", \"prediction\")\n",
    "\n",
    "pred.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering\n",
    "### Slide 1, page 53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "\n",
    "case class Nums(val1: Long, val2: Long, val3: Long)\n",
    "\n",
    "val numsDF = spark.createDataFrame(Seq(Nums(1, 2, 3), Nums(4, 5, 6), Nums(7, 8, 9))).toDF\n",
    "\n",
    "val va = new VectorAssembler().setInputCols(Array(\"val1\", \"val2\", \"val3\")).setOutputCol(\"features\")\n",
    "\n",
    "va.transform(numsDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering - continuous features\n",
    "### Slide 1, page 56-58"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.Bucketizer\n",
    "\n",
    "val contDF = spark.range(20).selectExpr(\"cast(id as double)\")\n",
    "val bucketBorders = Array(-1.0, 5.0, 10.0, 15.0, 20.0)\n",
    "val bucketer = new Bucketizer().setSplits(bucketBorders).setInputCol(\"id\")\n",
    "\n",
    "bucketer.transform(contDF).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "\n",
    "case class Nums(val1: Long, val2: Long, val3: Long)\n",
    "\n",
    "val numsDF = spark.createDataFrame(Seq(Nums(1, 2, 3), Nums(4, 5, 6), Nums(7, 8, 9))).toDF\n",
    "\n",
    "val va = new VectorAssembler().setInputCols(Array(\"val1\", \"val2\", \"val3\")).setOutputCol(\"features\")\n",
    "\n",
    "val nums = va.transform(numsDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.StandardScaler\n",
    "\n",
    "val scaler = new StandardScaler().setInputCol(\"features\").setOutputCol(\"scaled\")\n",
    "scaler.fit(nums).transform(nums).show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "\n",
    "case class Nums(val1: Long, val2: Long, val3: Long)\n",
    "\n",
    "val numsDF = spark.createDataFrame(Seq(Nums(1, 2, 3), Nums(4, 5, 6), Nums(7, 8, 9))).toDF\n",
    "\n",
    "val va = new VectorAssembler().setInputCols(Array(\"val1\", \"val2\", \"val3\")).setOutputCol(\"features\")\n",
    "\n",
    "val nums = va.transform(numsDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.MaxAbsScaler\n",
    "\n",
    "val maScaler = new MaxAbsScaler().setInputCol(\"features\").setOutputCol(\"mas\")\n",
    "maScaler.fit(nums).transform(nums).show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering - categorical features\n",
    "### Slide 1, page 60-62"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val simpleDF = spark.read.json(\"simple-ml.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.StringIndexer\n",
    "\n",
    "val lblIndxr = new StringIndexer().setInputCol(\"lab\").setOutputCol(\"labelInd\")\n",
    "val idxRes = lblIndxr.fit(simpleDF).transform(simpleDF)\n",
    "\n",
    "idxRes.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.IndexToString\n",
    "\n",
    "val labelReverse = new IndexToString().setInputCol(\"labelInd\").setOutputCol(\"original\")\n",
    "\n",
    "labelReverse.transform(idxRes).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.OneHotEncoder\n",
    "\n",
    "val lblIndxr = new StringIndexer().setInputCol(\"color\").setOutputCol(\"colorInd\")\n",
    "val colorLab = lblIndxr.fit(simpleDF).transform(simpleDF.select(\"color\"))\n",
    "val ohe = new OneHotEncoder().setInputCol(\"colorInd\").setOutputCol(\"one-hot\")\n",
    "\n",
    "ohe.transform(colorLab).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering - text data\n",
    "### Slide 1, page 64-66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val sales = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"sales.csv\").where(\"Description IS NOT NULL\")\n",
    "sales.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.Tokenizer\n",
    "\n",
    "val tkn = new Tokenizer().setInputCol(\"Description\").setOutputCol(\"DescOut\")\n",
    "val tokenized = tkn.transform(sales.select(\"Description\"))\n",
    "\n",
    "tokenized.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.StopWordsRemover\n",
    "\n",
    "val df = spark.createDataFrame(Seq((0, Seq(\"I\", \"saw\", \"the\", \"red\", \"balloon\")),\n",
    "    (1, Seq(\"Mary\", \"had\", \"a\", \"little\", \"lamb\")))).toDF(\"id\", \"raw\")\n",
    "\n",
    "val englishStopWords = StopWordsRemover.loadDefaultStopWords(\"english\")\n",
    "\n",
    "val stops = new StopWordsRemover().setStopWords(englishStopWords)\n",
    "    .setInputCol(\"raw\").setOutputCol(\"WithoutStops\")\n",
    "\n",
    "stops.transform(df).show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.CountVectorizer\n",
    "\n",
    "val df = spark.createDataFrame(Seq((0, Array(\"a\", \"b\", \"c\")),\n",
    "    (1, Array(\"a\", \"b\", \"b\", \"c\", \"a\")))).toDF(\"id\", \"words\")\n",
    "\n",
    "val cvModel = new CountVectorizer().setInputCol(\"words\").setOutputCol(\"features\").setVocabSize(3).setMinDF(2)\n",
    "val fittedCV = cvModel.fit(df)\n",
    "\n",
    "fittedCV.transform(df).show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression - normal equation\n",
    "### Slide 2, page 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case class house(x1: Long, x2: Long, y: Long)\n",
    "\n",
    "val trainData = spark.createDataFrame(Seq(house(2104, 3, 400), house(1600, 3, 330), house(2400, 3, 369),\n",
    "    house(1416, 2, 232), house(3000, 4, 540))).toDF\n",
    "\n",
    "val testData = spark.createDataFrame(Seq(house(4000, 4, 0))).toDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "\n",
    "val va = new VectorAssembler().setInputCols(Array(\"x1\", \"x2\")).setOutputCol(\"features\")\n",
    "\n",
    "val train = va.transform(trainData)\n",
    "val test = va.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.regression.LinearRegression\n",
    "\n",
    "val lr = new LinearRegression().setFeaturesCol(\"features\").setLabelCol(\"y\").setSolver(\"normal\")\n",
    "val lrModel = lr.fit(train)\n",
    "\n",
    "lrModel.transform(test).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression - gradient descent\n",
    "### Slide 2, page 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val data = spark.read.format(\"libsvm\").load(\"data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.regression.LinearRegression\n",
    "\n",
    "val lr = new LinearRegression().setMaxIter(10)\n",
    "val lrModel = lr.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(s\"Coefficients: ${lrModel.coefficients} Intercept: ${lrModel.intercept}\")\n",
    "\n",
    "val trainingSummary = lrModel.summary\n",
    "println(s\"RMSE: ${trainingSummary.rootMeanSquaredError}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression\n",
    "### Slide 3, page 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case class cancer(x1: Long, y: Long)\n",
    "\n",
    "val trainData = spark.createDataFrame(Seq(cancer(330, 1), cancer(120, 0), cancer(400, 1))).toDF\n",
    "val testData = spark.createDataFrame(Seq(cancer(500, 0))).toDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "\n",
    "val va = new VectorAssembler().setInputCols(Array(\"x1\")).setOutputCol(\"features\")\n",
    "val train = va.transform(trainData)\n",
    "val test = va.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "\n",
    "val lr = new LogisticRegression().setFeaturesCol(\"features\").setLabelCol(\"y\")\n",
    "    .setMaxIter(10).setRegParam(0.3).setElasticNetParam(0.8)\n",
    "val lrModel = lr.fit(train)\n",
    "\n",
    "lrModel.transform(test).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphX\n",
    "### Slide 4, page 52-54"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark._\n",
    "import org.apache.spark.graphx._\n",
    "import org.apache.spark.rdd.RDD\n",
    "val initialMsg = -9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val vertices: RDD[(VertexId, (Int, Int))] = sc.parallelize(Array((1L, (1, -1)),\n",
    "(2L, (2, -1)), (3L, (3, -1)), (6L, (6, -1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val relationships: RDD[Edge[Boolean]] = sc.parallelize(Array(Edge(1L, 2L, true),\n",
    "Edge(2L, 1L, true), Edge(2L, 6L, true), Edge(3L, 6L, true), Edge(6L, 1L, true),\n",
    "Edge(6L, 3L, true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val graph = Graph(vertices, relationships)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeMsg(msg1: Int, msg2: Int): Int = math.max(msg1, msg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vprog(vertexId: VertexId, value: (Int, Int), message: Int): (Int, Int) = {\n",
    "    if (message == initialMsg) // superstep 0\n",
    "        value\n",
    "    else // superstep > 0\n",
    "        (math.max(message, value._1), value._1) // return (newValue, oldValue)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sendMsg(triplet: EdgeTriplet[(Int, Int), Boolean]): Iterator[(VertexId, Int)] = {\n",
    "    val sourceVertex = triplet.srcAttr\n",
    "    if (sourceVertex._1 == sourceVertex._2) // newValue == oldValue for source vertex?\n",
    "        Iterator.empty // do nothing\n",
    "    else\n",
    "    // propogate new (updated) value to the destination vertex\n",
    "        Iterator((triplet.dstId, sourceVertex._1))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val minGraph = graph.pregel(initialMsg,\n",
    "    Int.MaxValue,\n",
    "    EdgeDirection.Out)(\n",
    "        vprog, // apply\n",
    "        sendMsg, // scatter\n",
    "        mergeMsg) // gather\n",
    "    minGraph.vertices.collect.foreach {\n",
    "        case (vertexId, (value, original_value)) => println(value)\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
